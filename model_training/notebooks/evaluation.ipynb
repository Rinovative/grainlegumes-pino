{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from src import analysis, util\n",
    "from src.analysis import evaluation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded278cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "model = \"FNO_lhs_var10_plog100_seed9_20251125_171807\"\n",
    "\n",
    "dataset_name_id = \"lhs_var10_plog100_seed9\"\n",
    "dataset_name_ood = \"lhs_var20_plog100_seed9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------\n",
    "checkpoint_path = Path(f\"../data/processed/{model}/best_model_state_dict.pt\")\n",
    "\n",
    "dataset_cases_path_id = Path(f\"../../data/raw/{dataset_name_id}/cases\")\n",
    "dataset_cases_path_ood = Path(f\"../../data/raw/{dataset_name_ood}/cases\") if dataset_name_ood else None\n",
    "\n",
    "save_root_id = Path(f\"../data/processed/{model}/analysis/id\")\n",
    "save_root_ood = Path(f\"../data/processed/{model}/analysis/ood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Helper: run only if parquet does NOT exist\n",
    "# ---------------------------------------------------------------------\n",
    "def run_or_load_artifacts_evaluation(\n",
    "    dataset_name: str,\n",
    "    save_root: Path,\n",
    "    dataset_path: Path,\n",
    ") -> tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    Load an existing Parquet artifact for the given dataset, or create it if missing.\n",
    "\n",
    "    This function checks whether the Parquet file for `dataset_name` already exists\n",
    "    in `save_root`. If it does, the DataFrame is loaded directly. If not, the full\n",
    "    inference pipeline is executed to generate:\n",
    "        - one NPZ file per case\n",
    "        - one Parquet file with global per-case statistics\n",
    "\n",
    "    Args:\n",
    "        dataset_name:\n",
    "            Name of the dataset (used for the Parquet filename).\n",
    "        save_root:\n",
    "            Directory where artifacts are stored (contains `<dataset>.parquet` and `npz/`).\n",
    "        dataset_path:\n",
    "            Path to the raw `cases/` folder of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, Path]:\n",
    "            df: Loaded or newly generated evaluation DataFrame.\n",
    "            parquet_path: Path to the Parquet artifact.\n",
    "\n",
    "    \"\"\"\n",
    "    parquet_path = save_root / f\"{dataset_name}.parquet\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # Fast path: Parquet already exists\n",
    "    # ------------------------------\n",
    "    if parquet_path.exists():\n",
    "        print(f\"[INFO] Found existing parquet → loading: {parquet_path}\")\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        return df, parquet_path\n",
    "\n",
    "    # ------------------------------\n",
    "    # Slow path: Generate artifacts\n",
    "    # ------------------------------\n",
    "    print(f\"[INFO] Creating artifacts for dataset: {dataset_name}\")\n",
    "\n",
    "    model, loader, processor, device = analysis.analysis_interference.load_inference_context(\n",
    "        dataset_path=dataset_path,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        batch_size=1,\n",
    "        ood_fraction=1.0,\n",
    "    )\n",
    "\n",
    "    df, parquet_path = analysis.analysis_artifacts.generate_artifacts(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        save_root=save_root,\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "\n",
    "    return df, parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# ID artifacts\n",
    "# ---------------------------------------------------------------------\n",
    "df_id, parquet_id = run_or_load_artifacts_evaluation(\n",
    "    dataset_name=dataset_name_id,\n",
    "    save_root=save_root_id,\n",
    "    dataset_path=dataset_cases_path_id,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# OOD artifacts (optional)\n",
    "# ---------------------------------------------------------------------\n",
    "df_ood = None\n",
    "\n",
    "if dataset_cases_path_ood is not None:\n",
    "    df_ood, parquet_ood = run_or_load_artifacts_evaluation(\n",
    "        dataset_name=dataset_name_ood,\n",
    "        save_root=save_root_ood,\n",
    "        dataset_path=dataset_cases_path_ood,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abc6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33094a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle = util.util_nb.make_toggle_shortcut(df_eval_id, dataset_name_id)\n",
    "\n",
    "global_error_analysis_plots = [\n",
    "    toggle(\"1-1. Global error metrics\", lambda: evaluation.evaluation_plots.plot_global_error_metrics(df_eval_id)),\n",
    "    toggle(\"1-2. Global error distribution (|U_error|)\", lambda: evaluation.evaluation_plots.plot_error_distribution(df_eval_id, field=\"U\")),\n",
    "    toggle(\"1-3. GT vs Prediction (U, global)\", lambda: evaluation.evaluation_plots.plot_global_gt_vs_pred(df_eval_id, field=\"U\")),\n",
    "    toggle(\"1-4. Mean error maps\", lambda: evaluation.evaluation_plots.plot_mean_error_maps(df_eval_id)),\n",
    "    toggle(\"1-5. Std error maps\", lambda: evaluation.evaluation_plots.plot_std_error_maps(df_eval_id)),\n",
    "]\n",
    "\n",
    "id_ood_comparison_plots = None\n",
    "\n",
    "if df_eval_ood is not None:\n",
    "    id_ood_comparison_plots = [\n",
    "        toggle(\"2-1. ID vs OOD metrics\", lambda: evaluation.evaluation_plots.plot_id_vs_ood_metrics(df_eval_id, df_eval_ood)),\n",
    "        toggle(\n",
    "            \"2-2. ID vs OOD error distributions (|U_error|)\",\n",
    "            lambda: evaluation.evaluation_plots.plot_id_vs_ood_error_distributions(df_eval_id, df_eval_ood),\n",
    "        ),\n",
    "        toggle(\"2-3. OOD - ID mean error map\", lambda: evaluation.evaluation_plots.plot_id_vs_ood_mean_error_difference(df_eval_id, df_eval_ood)),\n",
    "    ]\n",
    "\n",
    "permeability_sensitivity_plots = [\n",
    "    toggle(\"3-1. Error vs permeability magnitude\", lambda: evaluation.evaluation_plots.plot_error_vs_kappa_magnitude(df_eval_id)),\n",
    "    toggle(\"3-2. Error vs anisotropy ratio\", lambda: evaluation.evaluation_plots.plot_error_vs_anisotropy_ratio(df_eval_id)),\n",
    "    toggle(\"3-3. Error vs mean permeability\", lambda: evaluation.evaluation_plots.plot_error_vs_mean_kappa(df_eval_id)),\n",
    "    toggle(\"3-4. Error vs permeability gradient\", lambda: evaluation.evaluation_plots.plot_error_vs_kappa_gradient(df_eval_id)),\n",
    "]\n",
    "\n",
    "sample_viewer_plots = [\n",
    "    toggle(\"4-1. Sample Viewer — GT / Prediction / Error\", lambda: evaluation.evaluation_plots.plot_sample_prediction_overview(df_eval_id)),\n",
    "    toggle(\"4-2. Sample Viewer — kappa tensor (3×3) overlays\", lambda: evaluation.evaluation_plots.plot_sample_kappa_tensor_with_overlay(df_eval_id)),\n",
    "]\n",
    "\n",
    "\n",
    "sections = []\n",
    "\n",
    "# 1. Global Error Analysis\n",
    "sections.append(util.util_nb.make_dropdown_section(global_error_analysis_plots, f\"{dataset_name_id} — Global Error Analysis\"))\n",
    "\n",
    "# 2. ID/OOD Comparison (optional)\n",
    "if id_ood_comparison_plots is not None:\n",
    "    sections.append(util.util_nb.make_dropdown_section(id_ood_comparison_plots, f\"{dataset_name_id} vs {dataset_name_ood} — ID/OOD Comparison\"))\n",
    "\n",
    "# 3. Permeability Sensitivity\n",
    "sections.append(util.util_nb.make_dropdown_section(permeability_sensitivity_plots, f\"{dataset_name_id} — Permeability Sensitivity\"))\n",
    "\n",
    "# 4. Sample Viewer\n",
    "sections.append(util.util_nb.make_dropdown_section(sample_viewer_plots, f\"{dataset_name_id} — Sample Viewer\"))\n",
    "\n",
    "tab_titles = [\"1. Global Error Analysis\"]\n",
    "\n",
    "if id_ood_comparison_plots is not None:\n",
    "    tab_titles.append(\"2. ID/OOD Comparison\")\n",
    "    tab_titles.append(\"3. Permeability Sensitivity\")\n",
    "    tab_titles.append(\"4. Sample Viewer\")\n",
    "else:\n",
    "    tab_titles.append(\"2. Permeability Sensitivity\")\n",
    "    tab_titles.append(\"3. Sample Viewer\")\n",
    "\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{dataset_name_id} — Open Evaluation\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grainlegumes-pino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
