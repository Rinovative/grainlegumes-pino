{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b982c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from neuralop import H1Loss, LpLoss, Trainer\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from src import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è 0. Setup\n",
    "# ================================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üîë 1. W&B Setup\n",
    "# ================================================================\n",
    "os.environ[\"WANDB_API_KEY\"] = \"REMOVED_WANDB_KEY\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"grainlegumes_pino\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"Rinovative-Hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üì¶ 2. Dataset\n",
    "# ================================================================\n",
    "dataloader_cfg = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 8,\n",
    "    \"pin_memory\": True,\n",
    "    \"persistent_workers\": True,\n",
    "}\n",
    "\n",
    "train_loader, test_loaders, data_processor = dataset.dataset_base.create_dataloaders(\n",
    "    dataset_cls=dataset.dataset_simulation.PermeabilityFlowDataset,\n",
    "    path_train=\"../data/raw/lhs_var10_plog100_seed9/lhs_var10_plog100_seed9.pt\",\n",
    "    path_test_ood=\"../data/raw/lhs_var10_plog100_seed9/lhs_var10_plog100_seed9.pt\",\n",
    "    train_ratio=0.8,\n",
    "    ood_fraction=0.2,\n",
    "    **dataloader_cfg,\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# üîç Debug Info\n",
    "# ================================================================\n",
    "print(\"\\n=== Dataset Debug Info ===\")\n",
    "print(f\"Train loader size: {len(train_loader.dataset)} samples\")\n",
    "print(f\"Eval loader size:  {len(test_loaders['eval'].dataset)} samples\")\n",
    "print(f\"OOD loader size:   {len(test_loaders['ood'].dataset)} samples\")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch[\"x\"], batch[\"y\"]\n",
    "\n",
    "print(\"\\n--- First Batch ---\")\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"Global x mean/std: {x.mean():.4e} / {x.std():.4e}\")\n",
    "print(f\"Global y mean/std: {y.mean():.4e} / {y.std():.4e}\")\n",
    "\n",
    "# ================================================================\n",
    "# üîç Kanalweise Statistiken\n",
    "# ================================================================\n",
    "print(\"\\n--- Channel-wise Stats (x) ---\")\n",
    "for i in range(x.shape[1]):\n",
    "    print(f\"x[{i}]: mean={x[:, i].mean():.4e}, std={x[:, i].std():.4e}\")\n",
    "\n",
    "print(\"\\n--- Channel-wise Stats (y) ---\")\n",
    "for i in range(y.shape[1]):\n",
    "    print(f\"y[{i}]: mean={y[:, i].mean():.4e}, std={y[:, i].std():.4e}\")\n",
    "\n",
    "# ================================================================\n",
    "# üîç Normalizer Werte\n",
    "# ================================================================\n",
    "print(\"\\n--- Normalizer means/stds ---\")\n",
    "print(\"Input means:\", data_processor.in_normalizer.mean.flatten())\n",
    "print(\"Input stds: \", data_processor.in_normalizer.std.flatten())\n",
    "print(\"Output means:\", data_processor.out_normalizer.mean.flatten())\n",
    "print(\"Output stds: \", data_processor.out_normalizer.std.flatten())\n",
    "\n",
    "# ================================================================\n",
    "# üîç Zusatzcheck 1: Kanalbereich (Min/Max) zur Verifikation der Kanalzuordnung\n",
    "# ================================================================\n",
    "print(\"\\n--- Field ranges per channel (x) ---\")\n",
    "for i in range(x.shape[1]):\n",
    "    cmin, cmax = x[:, i].min().item(), x[:, i].max().item()\n",
    "    print(f\"x[{i}] range: min={cmin:.4f}, max={cmax:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# üîç Zusatzcheck 2: Normalisierung invertierbar?\n",
    "# ================================================================\n",
    "x_cpu = x.cpu()\n",
    "x_norm = data_processor.in_normalizer(x_cpu)\n",
    "\n",
    "mean = data_processor.in_normalizer.mean\n",
    "std = data_processor.in_normalizer.std\n",
    "\n",
    "# manuelles inverse-normalizing\n",
    "x_back = x_norm * std + mean\n",
    "\n",
    "err = (x_cpu - x_back).abs().mean().item()\n",
    "print(f\"\\nInvertibility check error: {err:.3e}\")\n",
    "\n",
    "# ================================================================\n",
    "# üîç Zusatzcheck 3: Wird im Loader wirklich normalisiert?\n",
    "# ================================================================\n",
    "raw_sample = next(iter(train_loader.dataset))[\"x\"]  # unnormalised\n",
    "norm_sample = x[0]  # first normalized sample\n",
    "\n",
    "print(\"\\n--- Raw vs Normalized sample check ---\")\n",
    "print(f\"Raw mean/std:    {raw_sample.mean():.4f} / {raw_sample.std():.4f}\")\n",
    "print(f\"Normed mean/std: {norm_sample.mean():.4f} / {norm_sample.std():.4f}\")\n",
    "\n",
    "print(\"=========================================================\\n\")\n",
    "\n",
    "data_processor = data_processor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee06d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üß† 3. Model\n",
    "# ================================================================\n",
    "model = FNO(\n",
    "    n_modes=(32, 32),\n",
    "    hidden_channels=64,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è 4. Optimizer, Scheduler, Loss\n",
    "# ================================================================\n",
    "optimizer = AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ü™Ñ 5. W&B Config & Init\n",
    "# ================================================================\n",
    "N_EPOCHS = 500\n",
    "\n",
    "config = {\n",
    "    \"model\": \"FNO\",\n",
    "    \"dataset\": \"PermeabilityFlow\",\n",
    "    \"batch_size\": dataloader_cfg[\"batch_size\"],\n",
    "    \"num_workers\": dataloader_cfg[\"num_workers\"],\n",
    "    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "    \"weight_decay\": optimizer.param_groups[0][\"weight_decay\"],\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"grainlegumes_pino\",\n",
    "    entity=\"Rinovative-Hub\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe63e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üöÄ 6. Trainer\n",
    "# ================================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    wandb_log=True,\n",
    "    device=device,\n",
    "    mixed_precision=False,\n",
    "    data_processor=data_processor,\n",
    "    eval_interval=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è 7. Training\n",
    "# ================================================================\n",
    "trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    test_loaders=test_loaders,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    "    save_best=\"eval_l2\",  # type: ignore[arg-type]\n",
    "    save_dir=\"../data/processed/model/test\",\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grainlegumes-pino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
