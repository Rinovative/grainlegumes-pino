{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec50461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import evaluation, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded278cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"FNO_uniform_var10_plog100_seed1_20251113_202554\"\n",
    "dataset_name_id = \"uniform_var10_plog100_seed1\"\n",
    "dataset_name_ood = \"uniform_var20_plog100_seed1\"  # or None if not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Dataset names and model checkpoint\n",
    "checkpoint_path = f\"model_training/data/processed/model/{model}/best_model_state_dict.pt\"\n",
    "dataset_pt_path_id = f\"model_training/data/raw/{dataset_name_id}/{dataset_name_id}.pt\"\n",
    "# Optional OOD dataset\n",
    "dataset_pt_path_ood = f\"model_training/data/raw/{dataset_name_ood}/{dataset_name_ood}.pt\" if dataset_name_ood is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inference context + build evaluation DataFrame (ID)\n",
    "model_id, loader_id, processor_id, device = evaluation.evaluation_inference.load_inference_context(\n",
    "    dataset_path=dataset_pt_path_id,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    batch_size=1,\n",
    "    ood_fraction=1.0,\n",
    ")\n",
    "\n",
    "df_eval_id = evaluation.evaluation_dataframe.create_evaluation_df(\n",
    "    model=model_id,\n",
    "    loader=loader_id,\n",
    "    processor=processor_id,\n",
    "    device=device,\n",
    "    dataset_name=dataset_name_id,\n",
    ")\n",
    "\n",
    "# Optional: Load OOD dataset + build OOD DF\n",
    "df_eval_ood = None\n",
    "\n",
    "if dataset_pt_path_ood is not None:\n",
    "    model_ood, loader_ood, processor_ood, _ = evaluation.evaluation_inference.load_inference_context(\n",
    "        dataset_path=dataset_pt_path_ood,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        batch_size=1,\n",
    "        ood_fraction=1.0,\n",
    "    )\n",
    "\n",
    "    df_eval_ood = evaluation.evaluation_dataframe.create_evaluation_df(\n",
    "        model=model_ood,\n",
    "        loader=loader_ood,\n",
    "        processor=processor_ood,\n",
    "        device=device,\n",
    "        dataset_name=dataset_name_ood,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33094a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle = util.util_nb.make_toggle_shortcut(df_eval_id, dataset_name_id)\n",
    "\n",
    "# Global Error Analysis\n",
    "tab1_plots = [\n",
    "    toggle(\"1-1. Global error metrics\", lambda: evaluation.evaluation_plots.plot_global_error_metrics(df_eval_id)),\n",
    "    toggle(\"1-2. Global error distribution (|U_error|)\", lambda: evaluation.evaluation_plots.plot_error_distribution(df_eval_id, field=\"U\")),\n",
    "    toggle(\"1-3. GT vs Prediction (U, global)\", lambda: evaluation.evaluation_plots.plot_global_gt_vs_pred(df_eval_id, field=\"U\")),\n",
    "    toggle(\"1-4. Mean error maps\", lambda: evaluation.evaluation_plots.plot_mean_error_maps(df_eval_id)),\n",
    "    toggle(\"1-5. Std error maps\", lambda: evaluation.evaluation_plots.plot_std_error_maps(df_eval_id)),\n",
    "]\n",
    "\n",
    "tab1_section = util.util_nb.make_dropdown_section(\n",
    "    tab1_plots,\n",
    "    f\"{dataset_name_id} - Global Error Analysis\",\n",
    ")\n",
    "\n",
    "# ID vs OOD Comparison (included only if OOD exists)\n",
    "tab2_section = None\n",
    "if df_eval_ood is not None:\n",
    "    tab2_plots = [\n",
    "        toggle(\"2-1. ID vs OOD metrics\", lambda: evaluation.evaluation_plots.plot_id_vs_ood_metrics(df_eval_id, df_eval_ood)),\n",
    "        toggle(\n",
    "            \"2-2. ID vs OOD error distributions (|U_error|)\",\n",
    "            lambda: evaluation.evaluation_plots.plot_id_vs_ood_error_distributions(df_eval_id, df_eval_ood),\n",
    "        ),\n",
    "        toggle(\"2-3. OOD - ID mean error map\", lambda: evaluation.evaluation_plots.plot_id_vs_ood_mean_error_difference(df_eval_id, df_eval_ood)),\n",
    "    ]\n",
    "\n",
    "    tab2_section = util.util_nb.make_dropdown_section(\n",
    "        tab2_plots,\n",
    "        f\"{dataset_name_id} vs {dataset_name_ood} - ID/OOD Comparison\",\n",
    "    )\n",
    "\n",
    "# Permeability Sensitivity (global)\n",
    "tab3_plots = [\n",
    "    toggle(\"3-1. Error vs permeability magnitude\", lambda: evaluation.evaluation_plots.plot_error_vs_kappa_magnitude(df_eval_id)),\n",
    "    toggle(\"3-2. Error vs anisotropy ratio\", lambda: evaluation.evaluation_plots.plot_error_vs_anisotropy_ratio(df_eval_id)),\n",
    "    toggle(\"3-3. Error vs mean permeability\", lambda: evaluation.evaluation_plots.plot_error_vs_mean_kappa(df_eval_id)),\n",
    "    toggle(\"3-4. Error vs permeability gradient\", lambda: evaluation.evaluation_plots.plot_error_vs_kappa_gradient(df_eval_id)),\n",
    "]\n",
    "\n",
    "tab3_section = util.util_nb.make_dropdown_section(\n",
    "    tab3_plots,\n",
    "    f\"{dataset_name_id} - Permeability Sensitivity\",\n",
    ")\n",
    "\n",
    "# Sample Viewer (Prediction/GT/Error + Tensor Overlay)\n",
    "tab4_plots = [\n",
    "    toggle(\"4-1. Sample Viewer — Prediction / GT / Error\", lambda: evaluation.evaluation_plots.plot_sample_prediction_overview(df_eval_id)),\n",
    "    toggle(\n",
    "        \"4-2. Sample Viewer — kappa tensor (3x3) with overlays\", lambda: evaluation.evaluation_plots.plot_sample_kappa_tensor_with_overlay(df_eval_id)\n",
    "    ),\n",
    "]\n",
    "\n",
    "tab4_section = util.util_nb.make_dropdown_section(\n",
    "    tab4_plots,\n",
    "    f\"{dataset_name_id} - Sample Viewer\",\n",
    ")\n",
    "\n",
    "sections = [tab1_section, tab3_section, tab4_section]\n",
    "tab_titles = [\n",
    "    \"1. Global Error Analysis\",\n",
    "    \"2. Permeability Sensitivity\",\n",
    "    \"3. Sample Viewer\",\n",
    "]\n",
    "\n",
    "# Insert ID/OOD tab if available\n",
    "if tab2_section is not None:\n",
    "    sections.insert(1, tab2_section)\n",
    "    tab_titles.insert(1, \"2. ID vs OOD\")\n",
    "\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{dataset_name_id} - Open Evaluation\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grainlegumes-pino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
