{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce56229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from src import analysis, util\n",
    "from src.analysis import evaluation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Configuration\n",
    "# ======================================================================\n",
    "model = \"FNO_lhs_var10_plog100_seed1_20251208_230304\"\n",
    "model = \"FNO_lhs_var20_plog100_seed1001_20251210_163827\"\n",
    "model = \"FNO_lhs_var40_plog100_seed2001_20251211_154034\"\n",
    "# model = \"FNO_lhs_var80_plog100_seed3001_20251212_125538\"\n",
    "\n",
    "parts = model.split(\"_\")\n",
    "dataset_name_id = \"_\".join(parts[1:5])\n",
    "\n",
    "dataset_names = [\n",
    "    dataset_name_id,\n",
    "    # \"lhs_var20_plog100_seed1001\",\n",
    "    # \"lhs_var40_plog100_seed2001\",\n",
    "    \"lhs_var80_plog100_seed3001\",\n",
    "    \"lhs_var160_plog100_seed4001\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea22802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------\n",
    "checkpoint_path = Path(f\"../data/processed/{model}/best_model_state_dict.pt\")\n",
    "\n",
    "# build cases paths automatically\n",
    "dataset_cases_paths = {name: Path(f\"../../data/raw/{name}/cases\") for name in dataset_names}\n",
    "\n",
    "# build save roots automatically\n",
    "save_roots = {dataset_names[0]: Path(f\"../data/processed/{model}/analysis/id\")}\n",
    "\n",
    "save_roots.update({name: Path(f\"../data/processed/{model}/analysis/ood\") / name for name in dataset_names[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_or_load_artifacts_evaluation(\n",
    "    dataset_name: str,\n",
    "    save_root: Path,\n",
    "    dataset_path: Path,\n",
    ") -> tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"Load an existing Parquet artifact for the given dataset, or create it if missing.\"\"\"\n",
    "    parquet_path = save_root / f\"{dataset_name}.parquet\"\n",
    "\n",
    "    if parquet_path.exists():\n",
    "        print(f\"[INFO] Found existing parquet -> loading: {parquet_path}\")\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"[INFO] Loaded df with {len(df)} rows\")\n",
    "        return df, parquet_path\n",
    "\n",
    "    print(f\"[INFO] Creating artifacts for: {dataset_name}\")\n",
    "\n",
    "    model, loader, processor, device = analysis.analysis_interference.load_inference_context(\n",
    "        dataset_path=dataset_path,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    df, parquet_path = analysis.analysis_artifacts.generate_artifacts(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        save_root=save_root,\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Artifact generation done.\")\n",
    "    return df, parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Generate artifacts\n",
    "# ---------------------------------------------------------------------\n",
    "datasets_raw = {}\n",
    "parquet_paths = {}\n",
    "\n",
    "for name in dataset_names:\n",
    "    df_raw, parquet_path = run_or_load_artifacts_evaluation(\n",
    "        dataset_name=name,\n",
    "        save_root=save_roots[name],\n",
    "        dataset_path=dataset_cases_paths[name],\n",
    "    )\n",
    "    datasets_raw[name] = df_raw\n",
    "    parquet_paths[name] = parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_eval = {name: evaluation.evaluation_dataframe.load_and_build_eval_df(parquet_paths[name]) for name in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e60c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# TOGGLE SETUP\n",
    "# =====================================================================\n",
    "\n",
    "toggle = util.util_nb.make_toggle_shortcut(dfs=datasets_eval)\n",
    "\n",
    "# =====================================================================\n",
    "# 3. SAMPLE VIEWER (nur 3-1 für Testing)\n",
    "# =====================================================================\n",
    "\n",
    "sample_viewer_plots = [\n",
    "    toggle(\"3-1. GT vs Prediction\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_prediction_overview),\n",
    "    toggle(\n",
    "        \"3-2. Kappa Tensor - Error Overlay\",\n",
    "        evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_kappa_tensor_with_overlay,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# SECTIONS UND TABS (nur Sample Viewer)\n",
    "# =====================================================================\n",
    "\n",
    "sections = [\n",
    "    util.util_nb.make_dropdown_section(sample_viewer_plots),\n",
    "]\n",
    "\n",
    "tab_titles = [\n",
    "    \"3. Sample Viewer\",\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# FINAL PANEL\n",
    "# =====================================================================\n",
    "\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{dataset_name_id} — Open Sample Viewer\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a43de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# TOGGLE SETUP\n",
    "# =====================================================================\n",
    "toggle = util.util_nb.make_toggle_shortcut(dfs=datasets_eval)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1. GLOBAL ERROR ANALYSIS\n",
    "# =====================================================================\n",
    "global_error_analysis_plots = [\n",
    "    toggle(\"1-1. Global error metrics\", evaluation.evaluation_plot.evaluation_plot_global_error_analysis.plot_global_error_metrics),\n",
    "    toggle(\"1-2. Global error distribution\", evaluation.evaluation_plot.evaluation_plot_global_error_analysis.plot_error_distribution),\n",
    "    toggle(\"1-3. GT vs Prediction\", evaluation.evaluation_plot.evaluation_plot_global_error_analysis.plot_global_gt_vs_pred),\n",
    "    toggle(\"1-4. Mean error maps\", evaluation.evaluation_plot.evaluation_plot_global_error_analysis.plot_mean_error_maps),\n",
    "    toggle(\"1-5. Std error maps\", evaluation.evaluation_plot.evaluation_plot_global_error_analysis.plot_std_error_maps),\n",
    "]\n",
    "\n",
    "\n",
    "# # =====================================================================\n",
    "# # 2. PERMEABILITY SENSITIVITY\n",
    "# # =====================================================================\n",
    "# permeability_sensitivity_plots = [\n",
    "#     toggle(\"2-1. Error vs permeability magnitude\", evaluation.evaluation_plot.evaluation_plot_permeability_sensitivity.plot_error_vs_kappa_magnitude), # noqa: E501\n",
    "#     toggle(\"2-2. Error vs anisotropy ratio\", evaluation.evaluation_plot.evaluation_plot_permeability_sensitivity.plot_error_vs_anisotropy_ratio),\n",
    "#     toggle(\"2-3. Error vs mean permeability\", evaluation.evaluation_plot.evaluation_plot_permeability_sensitivity.plot_error_vs_mean_kappa),\n",
    "#     toggle(\"2-4. Error vs permeability gradient\", evaluation.evaluation_plot.evaluation_plot_permeability_sensitivity.plot_error_vs_kappa_gradient),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # =====================================================================\n",
    "# # 3. SAMPLE VIEWER\n",
    "# # =====================================================================\n",
    "# sample_viewer_plots = [\n",
    "#     toggle(\"3-1. Sample Viewer — GT / Prediction / Error\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_prediction_overview), # noqa: E501\n",
    "#     toggle(\n",
    "#         \"3-2. Sample Viewer — kappa tensor overlays\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_kappa_tensor_with_overlay\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # =====================================================================\n",
    "# # 4. ID/OOD COMPARISON\n",
    "# # =====================================================================\n",
    "# id_ood_comparison_plots = None\n",
    "\n",
    "# if len(datasets_eval) > 1:\n",
    "#     id_ood_comparison_plots = [\n",
    "#         toggle(\"4-1. ID vs OOD metrics\", evaluation.evaluation_plot.evaluation_plot_id_vs_ood.plot_id_vs_ood_metrics),\n",
    "#         toggle(\"4-2. ID vs OOD error distributions\", evaluation.evaluation_plot.evaluation_plot_id_vs_ood.plot_id_vs_ood_error_distributions),\n",
    "#         toggle(\"4-3. OOD - ID mean error map\", evaluation.evaluation_plot.evaluation_plot_id_vs_ood.plot_id_vs_ood_mean_error_difference),\n",
    "#     ]\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# SECTIONS UND TABS\n",
    "# =====================================================================\n",
    "sections = [\n",
    "    util.util_nb.make_dropdown_section(global_error_analysis_plots),\n",
    "    # util.util_nb.make_dropdown_section(permeability_sensitivity_plots),\n",
    "    # util.util_nb.make_dropdown_section(sample_viewer_plots),\n",
    "]\n",
    "\n",
    "tab_titles = [\n",
    "    \"1. Global Error Analysis\",\n",
    "    # \"2. Permeability Sensitivity\",\n",
    "    # \"3. Sample Viewer\",\n",
    "]\n",
    "\n",
    "# if id_ood_comparison_plots is not None:\n",
    "#     sections.append(util.util_nb.make_dropdown_section(id_ood_comparison_plots))\n",
    "#     tab_titles.append(\"4. ID/OOD Comparison\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# FINAL PANEL\n",
    "# =====================================================================\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{model} - Open Evaluation\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grainlegumes-pino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
