{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from src import analysis, util\n",
    "from src.analysis import evaluation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "model = \"FNO_lhs_var10_plog100_seed9_20251205_155420\"\n",
    "\n",
    "dataset_name_id = \"lhs_var10_plog100_seed9\"\n",
    "dataset_name_ood = \"lhs_var20_plog100_seed9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------\n",
    "checkpoint_path = Path(f\"../data/processed/{model}/best_model_state_dict.pt\")\n",
    "\n",
    "dataset_cases_path_id = Path(f\"../../data/raw/{dataset_name_id}/cases\")\n",
    "dataset_cases_path_ood = Path(f\"../../data/raw/{dataset_name_ood}/cases\") if dataset_name_ood else None\n",
    "\n",
    "save_root_id = Path(f\"../data/processed/{model}/analysis/id\")\n",
    "save_root_ood = Path(f\"../data/processed/{model}/analysis/ood\") / dataset_name_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d86c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_or_load_artifacts_evaluation(\n",
    "    dataset_name: str,\n",
    "    save_root: Path,\n",
    "    dataset_path: Path,\n",
    ") -> tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"Load an existing Parquet artifact for the given dataset, or create it if missing.\"\"\"\n",
    "    parquet_path = save_root / f\"{dataset_name}.parquet\"\n",
    "\n",
    "    if parquet_path.exists():\n",
    "        print(f\"[INFO] Found existing parquet -> loading: {parquet_path}\")\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        print(f\"[INFO] Loaded df with {len(df)} rows\")\n",
    "        return df, parquet_path\n",
    "\n",
    "    print(f\"[INFO] Creating artifacts for dataset: {dataset_name}\")\n",
    "    print(f\"[INFO] save_root:    {save_root}\")\n",
    "    print(f\"[INFO] dataset_path: {dataset_path}\")\n",
    "    print(f\"[INFO] checkpoint:   {checkpoint_path}\")\n",
    "\n",
    "    model, loader, processor, device = analysis.analysis_interference.load_inference_context(\n",
    "        dataset_path=dataset_path,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        batch_size=1,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Inference context ready, starting artifact generation...\")\n",
    "    df, parquet_path = analysis.analysis_artifacts.generate_artifacts(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        save_root=save_root,\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Artifact generation done.\")\n",
    "    return df, parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# ID artifacts\n",
    "# ---------------------------------------------------------------------\n",
    "df_id, parquet_id = run_or_load_artifacts_evaluation(\n",
    "    dataset_name=dataset_name_id,\n",
    "    save_root=save_root_id,\n",
    "    dataset_path=dataset_cases_path_id,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# OOD artifacts (optional)\n",
    "# ---------------------------------------------------------------------\n",
    "df_ood = None\n",
    "\n",
    "if dataset_cases_path_ood is not None:\n",
    "    df_ood, parquet_ood = run_or_load_artifacts_evaluation(\n",
    "        dataset_name=dataset_name_ood,\n",
    "        save_root=save_root_ood,\n",
    "        dataset_path=dataset_cases_path_ood,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_id = evaluation.evaluation_dataframe.load_and_build_eval_df(parquet_id)\n",
    "df_eval_ood = None\n",
    "if dataset_cases_path_ood is not None:\n",
    "    df_eval_ood = evaluation.evaluation_dataframe.load_and_build_eval_df(parquet_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbf623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# TOGGLE SETUP\n",
    "# =====================================================================\n",
    "\n",
    "toggle = util.util_nb.make_toggle_shortcut(df=df_eval_ood, dataset_name=dataset_name_ood)\n",
    "\n",
    "# =====================================================================\n",
    "# 3. SAMPLE VIEWER (nur 3-1 für Testing)\n",
    "# =====================================================================\n",
    "\n",
    "sample_viewer_plots = [\n",
    "    toggle(\"3-1. GT vs Prediction\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_prediction_overview),\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# SECTIONS UND TABS (nur Sample Viewer)\n",
    "# =====================================================================\n",
    "\n",
    "sections = [\n",
    "    util.util_nb.make_dropdown_section(sample_viewer_plots),\n",
    "]\n",
    "\n",
    "tab_titles = [\n",
    "    \"3. Sample Viewer\",\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# FINAL PANEL\n",
    "# =====================================================================\n",
    "\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{dataset_name_id} — Open Sample Viewer\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# TOGGLE SETUP\n",
    "# =====================================================================\n",
    "toggle = util.util_nb.make_toggle_shortcut(\n",
    "    df=df_eval_id,\n",
    "    dataset_name=dataset_name_id,\n",
    "    df_alt=df_eval_ood,\n",
    "    dataset_name_alt=dataset_name_ood,\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1. GLOBAL ERROR ANALYSIS\n",
    "# =====================================================================\n",
    "global_error_analysis_plots = [\n",
    "    toggle(\"1-1. Global error metrics\", evaluation.evaluation_plot.plot_global_error_metrics),\n",
    "    toggle(\"1-2. Global error distribution\", evaluation.evaluation_plot.plot_error_distribution),\n",
    "    toggle(\"1-3. GT vs Prediction\", evaluation.evaluation_plot.plot_global_gt_vs_pred),\n",
    "    toggle(\"1-4. Mean error maps\", evaluation.evaluation_plot.plot_mean_error_maps),\n",
    "    toggle(\"1-5. Std error maps\", evaluation.evaluation_plot.plot_std_error_maps),\n",
    "]\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 2. PERMEABILITY SENSITIVITY\n",
    "# =====================================================================\n",
    "permeability_sensitivity_plots = [\n",
    "    toggle(\"2-1. Error vs permeability magnitude\", evaluation.evaluation_plot.plot_error_vs_kappa_magnitude),\n",
    "    toggle(\"2-2. Error vs anisotropy ratio\", evaluation.evaluation_plot.plot_error_vs_anisotropy_ratio),\n",
    "    toggle(\"2-3. Error vs mean permeability\", evaluation.evaluation_plot.plot_error_vs_mean_kappa),\n",
    "    toggle(\"2-4. Error vs permeability gradient\", evaluation.evaluation_plot.plot_error_vs_kappa_gradient),\n",
    "]\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3. SAMPLE VIEWER\n",
    "# =====================================================================\n",
    "sample_viewer_plots = [\n",
    "    toggle(\"3-1. Sample Viewer — GT / Prediction / Error\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_prediction_overview),\n",
    "    toggle(\n",
    "        \"3-2. Sample Viewer — kappa tensor overlays\", evaluation.evaluation_plot.evaluation_plot_sample_viewer.plot_sample_kappa_tensor_with_overlay\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 4. ID/OOD COMPARISON\n",
    "# =====================================================================\n",
    "id_ood_comparison_plots = None\n",
    "\n",
    "if df_eval_ood is not None:\n",
    "    id_ood_comparison_plots = [\n",
    "        toggle(\"4-1. ID vs OOD metrics\", evaluation.evaluation_plot.plot_id_vs_ood_metrics),\n",
    "        toggle(\"4-2. ID vs OOD error distributions\", evaluation.evaluation_plot.plot_id_vs_ood_error_distributions),\n",
    "        toggle(\"4-3. OOD - ID mean error map\", evaluation.evaluation_plot.plot_id_vs_ood_mean_error_difference),\n",
    "    ]\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# SECTIONS UND TABS\n",
    "# =====================================================================\n",
    "sections = [\n",
    "    util.util_nb.make_dropdown_section(global_error_analysis_plots, f\"{dataset_name_id} — Global Error Analysis\"),\n",
    "    util.util_nb.make_dropdown_section(permeability_sensitivity_plots, f\"{dataset_name_id} — Permeability Sensitivity\"),\n",
    "    util.util_nb.make_dropdown_section(sample_viewer_plots, f\"{dataset_name_id} — Sample Viewer\"),\n",
    "]\n",
    "\n",
    "if id_ood_comparison_plots is not None:\n",
    "    sections.append(util.util_nb.make_dropdown_section(id_ood_comparison_plots, f\"{dataset_name_id} vs {dataset_name_ood} — ID/OOD Comparison\"))\n",
    "\n",
    "\n",
    "tab_titles = [\n",
    "    \"1. Global Error Analysis\",\n",
    "    \"2. Permeability Sensitivity\",\n",
    "    \"3. Sample Viewer\",\n",
    "]\n",
    "\n",
    "if id_ood_comparison_plots is not None:\n",
    "    tab_titles.append(\"4. ID/OOD Comparison\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# FINAL PANEL\n",
    "# =====================================================================\n",
    "evaluation_panel = util.util_nb.make_lazy_panel_with_tabs(\n",
    "    sections,\n",
    "    tab_titles=tab_titles,\n",
    "    open_btn_text=f\"{dataset_name_id} — Open Evaluation\",\n",
    "    close_btn_text=\"Close\",\n",
    ")\n",
    "\n",
    "display(evaluation_panel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grainlegumes-pino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
